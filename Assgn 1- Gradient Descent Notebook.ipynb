{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading Libraries  and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: Matrix\n"
     ]
    }
   ],
   "source": [
    "library(optimbase)\n",
    "library(fastDummies)\n",
    "library(ISLR)\n",
    "fix(Auto)\n",
    "attach(Auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1, creating a function for NLL based Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientDescent<- function(X,y, weights,learning_rate, epoch){\n",
    "    m<- length(y)\n",
    "    \n",
    "    for (iter in 1:epoch){\n",
    "        sigma <- X %*% weights\n",
    "        sigmoid <- 1/(1+exp(-sigma))\n",
    "        error<- sigmoid-y\n",
    "        weights<- weights-(learning_rate/m)*(t(X)%*%error)\n",
    "        #if I use the below 2 lines instead of the one above, the function changes from NLL to MSE\n",
    "        #sigmoid_de<- sigmoid*(1-sigmoid)\n",
    "        #weights<- weights+2*((learning_rate/m)*(t(X)%*%(error*sigmoid_de)))\n",
    "        \n",
    "    }\n",
    "    return(weights)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2, Creating Input and Output variables, normalising and dummy variable creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "high<- (mpg>=25)\n",
    "y<-zeros(length(high),1)\n",
    "y[high]<-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results <- dummy_cols(origin, remove_first_dummy = TRUE)\n",
    "dataset<-data.frame(horsepower,weight,year)\n",
    "standardised.data<- scale(dataset[,1:3])\n",
    "X<- cbind(ones(nrow(y),1),standardised.data,results[,2],results[,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Important Parameter Values, and testing the Gradient descent on the whole training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights<- zeros(ncol(X),1)\n",
    "epoch <- 10000\n",
    "learning_rate <- 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights<-runif(ncol(X), min=-0.7, max=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights<-gradientDescent(X,y,weights,learning_rate,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><th scope=row></th><td>-2.6041703</td></tr>\n",
       "\t<tr><th scope=row>horsepower</th><td>-1.4361006</td></tr>\n",
       "\t<tr><th scope=row>weight</th><td>-4.0315939</td></tr>\n",
       "\t<tr><th scope=row>year</th><td> 1.7480174</td></tr>\n",
       "\t<tr><th scope=row></th><td> 1.1856663</td></tr>\n",
       "\t<tr><th scope=row></th><td> 0.1395751</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "\t & -2.6041703\\\\\n",
       "\thorsepower & -1.4361006\\\\\n",
       "\tweight & -4.0315939\\\\\n",
       "\tyear &  1.7480174\\\\\n",
       "\t &  1.1856663\\\\\n",
       "\t &  0.1395751\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "|  | -2.6041703 |\n",
       "| horsepower | -1.4361006 |\n",
       "| weight | -4.0315939 |\n",
       "| year |  1.7480174 |\n",
       "|  |  1.1856663 |\n",
       "|  |  0.1395751 |\n",
       "\n"
      ],
      "text/plain": [
       "           [,1]      \n",
       "           -2.6041703\n",
       "horsepower -1.4361006\n",
       "weight     -4.0315939\n",
       "year        1.7480174\n",
       "            1.1856663\n",
       "            0.1395751"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><th scope=row></th><td>-1.12944156</td></tr>\n",
       "\t<tr><th scope=row>horsepower</th><td>-0.75595791</td></tr>\n",
       "\t<tr><th scope=row>weight</th><td>-2.14877549</td></tr>\n",
       "\t<tr><th scope=row>year</th><td> 0.95047474</td></tr>\n",
       "\t<tr><th scope=row></th><td> 0.20151481</td></tr>\n",
       "\t<tr><th scope=row></th><td>-0.07429174</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "\t & -1.12944156\\\\\n",
       "\thorsepower & -0.75595791\\\\\n",
       "\tweight & -2.14877549\\\\\n",
       "\tyear &  0.95047474\\\\\n",
       "\t &  0.20151481\\\\\n",
       "\t & -0.07429174\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "|  | -1.12944156 |\n",
       "| horsepower | -0.75595791 |\n",
       "| weight | -2.14877549 |\n",
       "| year |  0.95047474 |\n",
       "|  |  0.20151481 |\n",
       "|  | -0.07429174 |\n",
       "\n"
      ],
      "text/plain": [
       "           [,1]       \n",
       "           -1.12944156\n",
       "horsepower -0.75595791\n",
       "weight     -2.14877549\n",
       "year        0.95047474\n",
       "            0.20151481\n",
       "           -0.07429174"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating prediction error on the training set and all misclassificatios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.2988072\n",
      "[1] 35\n"
     ]
    }
   ],
   "source": [
    "m<- length(y)\n",
    "h_x<- X%*%weights\n",
    "predict <- 1/(1+exp(-h_x))\n",
    "probs<- (predict>0.5)\n",
    "y_pred<-zeros(m,1)\n",
    "y_pred[probs]<-1\n",
    "print(((1/m)*sum((y-y_pred)^2))^0.5)\n",
    "print(sum(y_pred!= y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 3: Splitting the data into Training set and Test set (a split in half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "size <- floor(0.5 * nrow(X))\n",
    "set.seed(2810)\n",
    "train_ind <- sample(seq_len(nrow(X)), size = size)\n",
    "X_train <- X[train_ind, ]\n",
    "X_test <- X[-train_ind, ]\n",
    "y_train <- y[train_ind ]\n",
    "y_test <- y[-train_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 4: Running the algorithm and calculating calculating MSE on Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights<-runif(ncol(X), min=-0.7, max=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch <- 2000\n",
    "learning_rate <- 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights<-gradientDescent(X_train,y_train,initial_weights,learning_rate,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.3273268\n",
      "[1] 21\n"
     ]
    }
   ],
   "source": [
    "h_x<- X_train%*%weights\n",
    "predict <- 1/(1+exp(-h_x))\n",
    "probs<- (predict>0.5)\n",
    "y_pred<-zeros(length(y_train),1)\n",
    "y_pred[probs]<-1\n",
    "m<- length(y_train)\n",
    "print(((1/m)*sum((y_train-y_pred)^2))^0.5)\n",
    "print(sum(y_pred!= y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.3113499\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "19"
      ],
      "text/latex": [
       "19"
      ],
      "text/markdown": [
       "19"
      ],
      "text/plain": [
       "[1] 19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_x<- X_test%*%weights\n",
    "predict <- 1/(1+exp(-h_x))\n",
    "probs<- (predict>0.5)\n",
    "y_pred<-zeros(length(y_test),1)\n",
    "y_pred[probs]<-1\n",
    "m<- length(y_test)\n",
    "print(((1/m)*sum((y_test-y_pred)^2))^0.5)\n",
    "sum(y_pred!= y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><th scope=row></th><td>-1.8580643</td></tr>\n",
       "\t<tr><th scope=row>horsepower</th><td>-1.4308870</td></tr>\n",
       "\t<tr><th scope=row>weight</th><td>-3.5985392</td></tr>\n",
       "\t<tr><th scope=row>year</th><td> 1.6191479</td></tr>\n",
       "\t<tr><th scope=row></th><td> 0.7884743</td></tr>\n",
       "\t<tr><th scope=row></th><td> 0.3616122</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "\t & -1.8580643\\\\\n",
       "\thorsepower & -1.4308870\\\\\n",
       "\tweight & -3.5985392\\\\\n",
       "\tyear &  1.6191479\\\\\n",
       "\t &  0.7884743\\\\\n",
       "\t &  0.3616122\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "|  | -1.8580643 |\n",
       "| horsepower | -1.4308870 |\n",
       "| weight | -3.5985392 |\n",
       "| year |  1.6191479 |\n",
       "|  |  0.7884743 |\n",
       "|  |  0.3616122 |\n",
       "\n"
      ],
      "text/plain": [
       "           [,1]      \n",
       "           -1.8580643\n",
       "horsepower -1.4308870\n",
       "weight     -3.5985392\n",
       "year        1.6191479\n",
       "            0.7884743\n",
       "            0.3616122"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><th scope=row></th><td>-1.8580643</td></tr>\n",
       "\t<tr><th scope=row>horsepower</th><td>-1.4308870</td></tr>\n",
       "\t<tr><th scope=row>weight</th><td>-3.5985392</td></tr>\n",
       "\t<tr><th scope=row>year</th><td> 1.6191479</td></tr>\n",
       "\t<tr><th scope=row></th><td> 0.7884743</td></tr>\n",
       "\t<tr><th scope=row></th><td> 0.3616122</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "\t & -1.8580643\\\\\n",
       "\thorsepower & -1.4308870\\\\\n",
       "\tweight & -3.5985392\\\\\n",
       "\tyear &  1.6191479\\\\\n",
       "\t &  0.7884743\\\\\n",
       "\t &  0.3616122\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "|  | -1.8580643 |\n",
       "| horsepower | -1.4308870 |\n",
       "| weight | -3.5985392 |\n",
       "| year |  1.6191479 |\n",
       "|  |  0.7884743 |\n",
       "|  |  0.3616122 |\n",
       "\n"
      ],
      "text/plain": [
       "           [,1]      \n",
       "           -1.8580643\n",
       "horsepower -1.4308870\n",
       "weight     -3.5985392\n",
       "year        1.6191479\n",
       "            0.7884743\n",
       "            0.3616122"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 6:Running Gradient Descent 100 times with different weight initialisation, and Creating Box plot for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "m<- length(y_test)\n",
    "diff_error<- zeros(100,1)\n",
    "for (i in 1:100){\n",
    "    weights<-runif(ncol(X), min=-0.7, max=0.7)\n",
    "    weights<-gradientDescent(X_train,y_train,weights,learning_rate,epoch)\n",
    "    h_x<- X_test%*%weights\n",
    "    predict <- 1/(1+exp(-h_x))\n",
    "    probs<- (predict>0.5)\n",
    "    y_pred<-zeros(length(y_test),1)\n",
    "    y_pred[probs]<-1\n",
    "    diff_error[i]<-((1/m)*sum((y_test-y_pred)^2))^0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAACLlBMVEUAAAABAQECAgIDAwME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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxplot(diff_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Optional)Task 7:Running the Gradient Algorithm 4 times with different weights and choosing the prediction rule with the best MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_x<- X_train%*%weights\n",
    "predict <- 1/(1+exp(-h_x))\n",
    "probs<- (predict>0.5)\n",
    "y_pred<-zeros(length(y_train),1)\n",
    "y_pred[probs]<-1\n",
    "m<- length(y_train)\n",
    "print(((1/m)*sum((y_train-y_pred)^2))^0.5)\n",
    "print(sum(y_pred!= y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "m<- length(y_train)\n",
    "diff_error<- zeros(4,1)\n",
    "diff_weights <-zeros(ncol(X_train),4)\n",
    "#learning_rate<-0.3; They were already set\n",
    "#epoch<-2000 ; They were already set\n",
    "for (i in 1:4){\n",
    "    weights<-runif(ncol(X_train), min=-0.7, max=0.7)\n",
    "    diff_weights[,i]<-gradientDescent(X_train,y_train,weights,learning_rate,epoch)\n",
    "    h_x<- X_train%*%diff_weights[,i]\n",
    "    predict <- 1/(1+exp(-h_x))\n",
    "    probs<- (predict>0.5)\n",
    "    y_pred<-zeros(length(y_train),1)\n",
    "    y_pred[probs]<-1\n",
    "    diff_error[i]<-((1/m)*sum((y_train-y_pred)^2))^0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>0.3273268</td></tr>\n",
       "\t<tr><td>0.3273268</td></tr>\n",
       "\t<tr><td>0.3273268</td></tr>\n",
       "\t<tr><td>0.3273268</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{l}\n",
       "\t 0.3273268\\\\\n",
       "\t 0.3273268\\\\\n",
       "\t 0.3273268\\\\\n",
       "\t 0.3273268\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 0.3273268 |\n",
       "| 0.3273268 |\n",
       "| 0.3273268 |\n",
       "| 0.3273268 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]     \n",
       "[1,] 0.3273268\n",
       "[2,] 0.3273268\n",
       "[3,] 0.3273268\n",
       "[4,] 0.3273268"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>-1.8591098</td><td>-1.8597288</td><td>-1.8580603</td><td>-1.8628514</td></tr>\n",
       "\t<tr><td>-1.4674338</td><td>-1.4422740</td><td>-1.4428050</td><td>-1.4471293</td></tr>\n",
       "\t<tr><td>-3.5641617</td><td>-3.5879352</td><td>-3.5866813</td><td>-3.5850146</td></tr>\n",
       "\t<tr><td> 1.6099637</td><td> 1.6167663</td><td> 1.6159335</td><td> 1.6168322</td></tr>\n",
       "\t<tr><td> 0.7797622</td><td> 0.7883123</td><td> 0.7853562</td><td> 0.7919861</td></tr>\n",
       "\t<tr><td> 0.3747738</td><td> 0.3681478</td><td> 0.3659088</td><td> 0.3738828</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{llll}\n",
       "\t -1.8591098 & -1.8597288 & -1.8580603 & -1.8628514\\\\\n",
       "\t -1.4674338 & -1.4422740 & -1.4428050 & -1.4471293\\\\\n",
       "\t -3.5641617 & -3.5879352 & -3.5866813 & -3.5850146\\\\\n",
       "\t  1.6099637 &  1.6167663 &  1.6159335 &  1.6168322\\\\\n",
       "\t  0.7797622 &  0.7883123 &  0.7853562 &  0.7919861\\\\\n",
       "\t  0.3747738 &  0.3681478 &  0.3659088 &  0.3738828\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| -1.8591098 | -1.8597288 | -1.8580603 | -1.8628514 |\n",
       "| -1.4674338 | -1.4422740 | -1.4428050 | -1.4471293 |\n",
       "| -3.5641617 | -3.5879352 | -3.5866813 | -3.5850146 |\n",
       "|  1.6099637 |  1.6167663 |  1.6159335 |  1.6168322 |\n",
       "|  0.7797622 |  0.7883123 |  0.7853562 |  0.7919861 |\n",
       "|  0.3747738 |  0.3681478 |  0.3659088 |  0.3738828 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]       [,2]       [,3]       [,4]      \n",
       "[1,] -1.8591098 -1.8597288 -1.8580603 -1.8628514\n",
       "[2,] -1.4674338 -1.4422740 -1.4428050 -1.4471293\n",
       "[3,] -3.5641617 -3.5879352 -3.5866813 -3.5850146\n",
       "[4,]  1.6099637  1.6167663  1.6159335  1.6168322\n",
       "[5,]  0.7797622  0.7883123  0.7853562  0.7919861\n",
       "[6,]  0.3747738  0.3681478  0.3659088  0.3738828"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights<- diff_weights[,which.min(diff_error)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>-1.85910975261121</li>\n",
       "\t<li>-1.46743377649456</li>\n",
       "\t<li>-3.56416166881255</li>\n",
       "\t<li>1.60996373336967</li>\n",
       "\t<li>0.779762190888072</li>\n",
       "\t<li>0.374773757042767</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item -1.85910975261121\n",
       "\\item -1.46743377649456\n",
       "\\item -3.56416166881255\n",
       "\\item 1.60996373336967\n",
       "\\item 0.779762190888072\n",
       "\\item 0.374773757042767\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. -1.85910975261121\n",
       "2. -1.46743377649456\n",
       "3. -3.56416166881255\n",
       "4. 1.60996373336967\n",
       "5. 0.779762190888072\n",
       "6. 0.374773757042767\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] -1.8591098 -1.4674338 -3.5641617  1.6099637  0.7797622  0.3747738"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Misclassifications and test error on our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.3113499\n",
      "[1] 19\n"
     ]
    }
   ],
   "source": [
    "h_x<- X_test%*%best_weights\n",
    "predict <- 1/(1+exp(-h_x))\n",
    "probs<- (predict>0.5)\n",
    "y_pred<-zeros(length(y_test),1)\n",
    "y_pred[probs]<-1\n",
    "m<- length(y_test)\n",
    "print(((1/m)*sum((y_test-y_pred)^2))^0.5)\n",
    "print(sum(y_pred!= y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Optional) Task 5: Trying different stopping rule- Stopping the algorithm if the reduction in Loss Function is less than a chosen\n",
    "#but small value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientDescent_new<- function(X,y, weights,learning_rate, epoch){\n",
    "    m<- length(y)\n",
    "\n",
    "    J_history<- zeros(epoch,1)\n",
    "    \n",
    "    for (iter in 1:epoch){\n",
    "        sigma = X %*% weights\n",
    "        sigmoid = 1/(1+exp(-sigma))\n",
    "        sigmoid_de<- sigmoid*(1-sigmoid)\n",
    "        error<- y-sigmoid\n",
    "        weights<- weights+2*((learning_rate/m)*(t(X)%*%(error*sigmoid_de)))\n",
    "        J_history[iter]<-CostFunction(X,y,weights)\n",
    "        if (iter>1 &&( J_history[iter-1]-J_history[iter]<0.00001)){\n",
    "        break\n",
    "        }\n",
    "        \n",
    "    }\n",
    "    print(iter)\n",
    "    return(weights)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "CostFunction<- function(X , y,weights){\n",
    "    m <- length(y)\n",
    "    J<-0\n",
    "    grad<-zeros(length(weights),1)\n",
    "    z<-(X%*%weights)\n",
    "    sigmoid <- 1/(1+exp(-z))\n",
    "    J<- (1/m)*sum((-y*log(sigmoid))-((1-y)*log(1-sigmoid)))\n",
    "    return(J)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1094\n",
      "[1] 0.3273268\n",
      "[1] 21\n"
     ]
    }
   ],
   "source": [
    "weights<-gradientDescent_new(X_train,y_train,initial_weights,learning_rate,epoch)\n",
    "h_x<- X_train%*%weights\n",
    "predict <- 1/(1+exp(-h_x))\n",
    "probs<- (predict>0.5)\n",
    "y_pred<-zeros(length(y_train),1)\n",
    "y_pred[probs]<-1\n",
    "m<- length(y_train)\n",
    "print(((1/m)*sum((y_train-y_pred)^2))^0.5)\n",
    "print(sum(y_pred!= y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.3350297\n",
      "[1] 22\n"
     ]
    }
   ],
   "source": [
    "h_x<- X_test%*%weights\n",
    "predict <- 1/(1+exp(-h_x))\n",
    "probs<- (predict>0.5)\n",
    "y_pred<-zeros(length(y_test),1)\n",
    "y_pred[probs]<-1\n",
    "m<- length(y_test)\n",
    "print(((1/m)*sum((y_test-y_pred)^2))^0.5)\n",
    "print(sum(y_pred!= y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
